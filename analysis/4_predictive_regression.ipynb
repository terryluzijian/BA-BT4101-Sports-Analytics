{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictive Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import cmocean\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Read data from the data folder\n",
    "race_df = pd.read_csv('data/race.csv', low_memory=False, index_col=0)\n",
    "horse_df = pd.read_csv('data/horse.csv', low_memory=False, index_col=0)\n",
    "individual_df = pd.read_csv('data/individual.csv', low_memory=False, index_col=0)\n",
    "trainer_df = pd.read_csv('data/trainer.csv', low_memory=False, index_col=0)\n",
    "jockey_df = pd.read_csv('data/jockey.csv', low_memory=False, index_col=0)\n",
    "horse_race_df = pd.read_csv('data/horse_race.csv', low_memory=False, index_col=0)\n",
    "horse_race_df['age_int'] = horse_race_df['sex_age'].apply(lambda x: re.search(r'\\d+', x).group(0)).astype(int)\n",
    "\n",
    "# Do some simple data transformation\n",
    "horse_race_df['run_date'] = horse_race_df['run_date'].apply(pd.Timestamp)\n",
    "horse_race_df = horse_race_df.sort_values(['horse_id', 'run_date'])\n",
    "try:\n",
    "    first_occur_df = pd.read_csv('data/first_occurence_race.csv', low_memory=False, index_col=0)\n",
    "    first_occur_df['run_date'] = first_occur_df['run_date'].apply(pd.Timestamp)\n",
    "except FileNotFoundError:\n",
    "    horse_race_sorted = horse_race_df.copy()\n",
    "    horse_id_set = set()\n",
    "    first_occur_dict = {}\n",
    "    for index, value in horse_race_sorted.iterrows():\n",
    "        if value['horse_id'] not in horse_id_set:\n",
    "            horse_id_set.add(value['horse_id'])\n",
    "            first_occur_dict[index] = value\n",
    "    first_occur_df = pd.DataFrame.from_dict(first_occur_dict, orient='index')\n",
    "    first_occur_df.to_csv('data/first_occurence_race.csv', encoding='utf-8')\n",
    "    \n",
    "columns_to_drop = [\n",
    "    'race', 'title', 'horse', 'sex_age',\n",
    "    'distance', 'run_time', 'breeder',\n",
    "    'jockey', 'margin', 'trainer_x', 'trainer_y', 'owner_x', 'owner_y', 'horse_name', 'date_of_birth', \n",
    "    'transaction_price', 'prize_obtained', 'race_record', 'highlight_race', 'relatives', 'status', 'prize'\n",
    "]\n",
    "for column in columns_to_drop:\n",
    "    try:\n",
    "        first_occur_df.drop(column, axis=1, inplace=True)\n",
    "        horse_race_df.drop(column, axis=1, inplace=True)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "horse_race_df = horse_race_df[horse_race_df['finishing_position'].apply(lambda x: bool(re.search(r'\\d+', x)))]\n",
    "horse_race_df['finishing_position'] = horse_race_df['finishing_position'].apply(lambda x: re.search(r'\\d+', x).group(0))\n",
    "horse_race_df['finishing_position'] = horse_race_df['finishing_position'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_order_by_count(df, column_name):\n",
    "    # Get dummies by descending count order\n",
    "    return pd.get_dummies(df[column_name]).reindex(df[column_name].value_counts().index, axis=1).iloc[:, :-1]\n",
    "\n",
    "def parse_time_stamp(time_string):\n",
    "    # Parse timestamp expressed in hours\n",
    "    time_split = time_string.split(':')\n",
    "    hour = int(time_split[0])\n",
    "    if hour < 12:\n",
    "        return '10-12'\n",
    "    elif hour > 12 and hour < 15:\n",
    "        return '12-15'\n",
    "    else:\n",
    "        return '15-after'\n",
    "    \n",
    "def get_trainer_jockey_profile(df, individual):\n",
    "    # Merge with trainer/jockey dataframe\n",
    "    assert individual in ['trainer', 'jockey']\n",
    "    if individual == 'trainer':\n",
    "        merge_df = trainer_df\n",
    "    elif individual == 'jockey':\n",
    "        merge_df = jockey_df\n",
    "    df = df.merge(merge_df[['%s_id' % individual, 'date_of_birth', 'place_of_birth']], \n",
    "                  on='%s_id' % individual, suffixes=['', '_%s' % individual])\n",
    "    df['run_date'] = df['run_date'].apply(lambda x: pd.Timestamp(x))\n",
    "    df['date_of_birth'] = df['date_of_birth'].apply(lambda x: pd.Timestamp(x))\n",
    "    df['%s_age' % individual] = df['run_date'].subtract(df['date_of_birth']).dt.days / 365.0\n",
    "    df.drop(['date_of_birth'], axis=1, inplace=True)\n",
    "    df['place_of_birth_%s' % individual] = df['place_of_birth_%s' % individual].apply(lambda x: 'tokyo' if x == u'東京都' \\\n",
    "                                                                                      else 'outside_tokyo')\n",
    "    return df\n",
    "\n",
    "def feature_engineer(race_df, dummy=True, drop_columns=True):\n",
    "    \n",
    "    new_df = race_df.copy()\n",
    "\n",
    "    # Feature engineering\n",
    "    has_horse_weight = new_df['horse_weight'].apply(lambda x: bool(re.search(r'(\\d+)\\(.+\\)', x)))\n",
    "    new_df = new_df[has_horse_weight]\n",
    "    new_df['horse_weight_increase'] = new_df['horse_weight'].apply(lambda x: re.search(r'\\(.?(\\d+)\\)', x).group(1))\n",
    "    new_df['horse_weight_increase'] = new_df['horse_weight_increase'].astype(float)\n",
    "    new_df['horse_weight'] = new_df['horse_weight'].apply(lambda x: re.search(r'(\\d+)\\(.+\\)', x).group(1))\n",
    "    new_df['horse_weight'] = new_df['horse_weight'].astype(float)\n",
    "\n",
    "    new_df['time'] = new_df['time'].apply(lambda x: parse_time_stamp(x))\n",
    "\n",
    "    for individual in ['jockey', 'trainer']:\n",
    "        new_df = get_trainer_jockey_profile(new_df, individual)\n",
    "\n",
    "    # Get dummy columns\n",
    "    if dummy:\n",
    "        dummied_cols = ['place', 'type', 'track', 'weather', 'condition', 'gender', 'breed', 'bracket', 'horse_number', \n",
    "                        'time', 'place_of_birth_jockey', 'place_of_birth_trainer']\n",
    "        for cols in dummied_cols:\n",
    "            new_df = new_df.join(get_dummies_order_by_count(new_df, \n",
    "                                                           cols).rename(columns=lambda x: '-'.join([cols, str(x)])))\n",
    "            try:\n",
    "                new_df.drop(cols, axis=1, inplace=True)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Drop some other columns\n",
    "    columns_to_drop_again = ['finishing_position', 'corner_position', 'run_time_last_600', \n",
    "                             'jockey_id', 'owner_id', 'trainer_id', 'breeder_id', \n",
    "                             'parents', 'age_int', 'place_of_birth']\n",
    "    if drop_columns:\n",
    "        for cols in columns_to_drop_again:\n",
    "            try:\n",
    "                new_df.drop(cols, axis=1, inplace=True)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    return new_df.sort_values(['horse_id', 'run_date']).set_index(['horse_id', 'run_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 OLS for First Occurence Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>run_time_1000</td>  <th>  R-squared:         </th>  <td>   0.426</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.425</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   971.6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:35:56</td>     <th>  Log-Likelihood:    </th> <td>-1.7226e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 86622</td>      <th>  AIC:               </th>  <td>3.446e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 86555</td>      <th>  BIC:               </th>  <td>3.453e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    66</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                    <td></td>                      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                                <td>   60.2242</td> <td>    0.633</td> <td>   95.151</td> <td> 0.000</td> <td>   58.984</td> <td>   61.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jockey_weight</th>                        <td>    0.0838</td> <td>    0.007</td> <td>   12.578</td> <td> 0.000</td> <td>    0.071</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>win_odds</th>                             <td>    0.0003</td> <td>    0.000</td> <td>    2.471</td> <td> 0.013</td> <td> 5.54e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>win_fav</th>                              <td>    0.1028</td> <td>    0.002</td> <td>   45.776</td> <td> 0.000</td> <td>    0.098</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_weight</th>                         <td>    0.0074</td> <td>    0.000</td> <td>   33.384</td> <td> 0.000</td> <td>    0.007</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>curr_age</th>                             <td>   -0.2335</td> <td>    0.013</td> <td>  -17.429</td> <td> 0.000</td> <td>   -0.260</td> <td>   -0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_weight_increase</th>                <td>   -0.0534</td> <td>    0.002</td> <td>  -25.199</td> <td> 0.000</td> <td>   -0.058</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jockey_age</th>                           <td>   -0.0042</td> <td>    0.001</td> <td>   -4.859</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trainer_age</th>                          <td>   -0.0024</td> <td>    0.001</td> <td>   -3.542</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-中山</th>                             <td>    0.9802</td> <td>    0.037</td> <td>   26.680</td> <td> 0.000</td> <td>    0.908</td> <td>    1.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-京都</th>                             <td>    0.2527</td> <td>    0.036</td> <td>    6.929</td> <td> 0.000</td> <td>    0.181</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-東京</th>                             <td>    0.1641</td> <td>    0.261</td> <td>    0.628</td> <td> 0.530</td> <td>   -0.348</td> <td>    0.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-阪神</th>                             <td>    0.3444</td> <td>    0.037</td> <td>    9.277</td> <td> 0.000</td> <td>    0.272</td> <td>    0.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-新潟</th>                             <td>   -0.1590</td> <td>    0.262</td> <td>   -0.606</td> <td> 0.545</td> <td>   -0.673</td> <td>    0.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-小倉</th>                             <td>   -0.3745</td> <td>    0.040</td> <td>   -9.347</td> <td> 0.000</td> <td>   -0.453</td> <td>   -0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-福島</th>                             <td>    0.2976</td> <td>    0.041</td> <td>    7.310</td> <td> 0.000</td> <td>    0.218</td> <td>    0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-中京</th>                             <td>    0.0286</td> <td>    0.263</td> <td>    0.109</td> <td> 0.913</td> <td>   -0.486</td> <td>    0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-札幌</th>                             <td>    0.3523</td> <td>    0.044</td> <td>    8.007</td> <td> 0.000</td> <td>    0.266</td> <td>    0.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type-芝</th>                               <td>   -5.5165</td> <td>    0.100</td> <td>  -54.938</td> <td> 0.000</td> <td>   -5.713</td> <td>   -5.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type-ダ</th>                               <td>   -3.0927</td> <td>    0.100</td> <td>  -30.953</td> <td> 0.000</td> <td>   -3.289</td> <td>   -2.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-右</th>                              <td>   -2.1478</td> <td>    0.135</td> <td>  -15.903</td> <td> 0.000</td> <td>   -2.413</td> <td>   -1.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-左</th>                              <td>   -1.7711</td> <td>    0.143</td> <td>  -12.428</td> <td> 0.000</td> <td>   -2.050</td> <td>   -1.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-直</th>                              <td>   -4.6903</td> <td>    0.160</td> <td>  -29.390</td> <td> 0.000</td> <td>   -5.003</td> <td>   -4.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-晴</th>                            <td>   -1.0084</td> <td>    0.203</td> <td>   -4.959</td> <td> 0.000</td> <td>   -1.407</td> <td>   -0.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-曇</th>                            <td>   -1.0341</td> <td>    0.204</td> <td>   -5.081</td> <td> 0.000</td> <td>   -1.433</td> <td>   -0.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-雨</th>                            <td>   -0.5828</td> <td>    0.205</td> <td>   -2.842</td> <td> 0.004</td> <td>   -0.985</td> <td>   -0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-小雨</th>                           <td>   -0.8757</td> <td>    0.206</td> <td>   -4.259</td> <td> 0.000</td> <td>   -1.279</td> <td>   -0.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-雪</th>                            <td>    0.0611</td> <td>    0.280</td> <td>    0.218</td> <td> 0.827</td> <td>   -0.488</td> <td>    0.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-良</th>                          <td>   -0.0470</td> <td>    0.033</td> <td>   -1.426</td> <td> 0.154</td> <td>   -0.112</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-稍重</th>                         <td>    0.1928</td> <td>    0.035</td> <td>    5.508</td> <td> 0.000</td> <td>    0.124</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-重</th>                          <td>   -0.0372</td> <td>    0.038</td> <td>   -0.971</td> <td> 0.332</td> <td>   -0.112</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender-牡</th>                             <td>   -0.0481</td> <td>    0.030</td> <td>   -1.588</td> <td> 0.112</td> <td>   -0.107</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender-牝</th>                             <td>   -0.2112</td> <td>    0.032</td> <td>   -6.628</td> <td> 0.000</td> <td>   -0.274</td> <td>   -0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-鹿毛</th>                             <td>   -0.6932</td> <td>    0.417</td> <td>   -1.662</td> <td> 0.097</td> <td>   -1.511</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-栗毛</th>                             <td>   -0.7253</td> <td>    0.417</td> <td>   -1.738</td> <td> 0.082</td> <td>   -1.543</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-黒鹿毛</th>                            <td>   -0.6463</td> <td>    0.417</td> <td>   -1.549</td> <td> 0.121</td> <td>   -1.464</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-芦毛</th>                             <td>   -0.7116</td> <td>    0.418</td> <td>   -1.703</td> <td> 0.089</td> <td>   -1.531</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-青鹿毛</th>                            <td>   -0.5769</td> <td>    0.418</td> <td>   -1.380</td> <td> 0.167</td> <td>   -1.396</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-青毛</th>                             <td>   -0.5803</td> <td>    0.421</td> <td>   -1.380</td> <td> 0.168</td> <td>   -1.404</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-栃栗毛</th>                            <td>   -0.5953</td> <td>    0.426</td> <td>   -1.399</td> <td> 0.162</td> <td>   -1.429</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-8</th>                            <td>    1.9782</td> <td>    0.071</td> <td>   27.874</td> <td> 0.000</td> <td>    1.839</td> <td>    2.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-7</th>                            <td>    1.7362</td> <td>    0.069</td> <td>   25.326</td> <td> 0.000</td> <td>    1.602</td> <td>    1.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-6</th>                            <td>    1.5205</td> <td>    0.066</td> <td>   22.958</td> <td> 0.000</td> <td>    1.391</td> <td>    1.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-5</th>                            <td>    1.3159</td> <td>    0.064</td> <td>   20.485</td> <td> 0.000</td> <td>    1.190</td> <td>    1.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-4</th>                            <td>    1.0767</td> <td>    0.061</td> <td>   17.756</td> <td> 0.000</td> <td>    0.958</td> <td>    1.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-3</th>                            <td>    0.8095</td> <td>    0.057</td> <td>   14.098</td> <td> 0.000</td> <td>    0.697</td> <td>    0.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-2</th>                            <td>    0.4183</td> <td>    0.046</td> <td>    9.126</td> <td> 0.000</td> <td>    0.328</td> <td>    0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-6</th>                       <td>    1.1650</td> <td>    0.092</td> <td>   12.636</td> <td> 0.000</td> <td>    0.984</td> <td>    1.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-2</th>                       <td>    2.0629</td> <td>    0.103</td> <td>   19.947</td> <td> 0.000</td> <td>    1.860</td> <td>    2.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-4</th>                       <td>    1.5230</td> <td>    0.095</td> <td>   15.958</td> <td> 0.000</td> <td>    1.336</td> <td>    1.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-3</th>                       <td>    1.7055</td> <td>    0.098</td> <td>   17.491</td> <td> 0.000</td> <td>    1.514</td> <td>    1.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-1</th>                       <td>    2.3276</td> <td>    0.112</td> <td>   20.699</td> <td> 0.000</td> <td>    2.107</td> <td>    2.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-5</th>                       <td>    1.2813</td> <td>    0.093</td> <td>   13.742</td> <td> 0.000</td> <td>    1.099</td> <td>    1.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-8</th>                       <td>    0.9185</td> <td>    0.090</td> <td>   10.187</td> <td> 0.000</td> <td>    0.742</td> <td>    1.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-7</th>                       <td>    1.0087</td> <td>    0.091</td> <td>   11.074</td> <td> 0.000</td> <td>    0.830</td> <td>    1.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-9</th>                       <td>    0.7936</td> <td>    0.090</td> <td>    8.858</td> <td> 0.000</td> <td>    0.618</td> <td>    0.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-10</th>                      <td>    0.6795</td> <td>    0.089</td> <td>    7.612</td> <td> 0.000</td> <td>    0.505</td> <td>    0.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-11</th>                      <td>    0.5423</td> <td>    0.089</td> <td>    6.085</td> <td> 0.000</td> <td>    0.368</td> <td>    0.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-12</th>                      <td>    0.4920</td> <td>    0.089</td> <td>    5.520</td> <td> 0.000</td> <td>    0.317</td> <td>    0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-13</th>                      <td>    0.4292</td> <td>    0.089</td> <td>    4.802</td> <td> 0.000</td> <td>    0.254</td> <td>    0.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-14</th>                      <td>    0.3364</td> <td>    0.090</td> <td>    3.753</td> <td> 0.000</td> <td>    0.161</td> <td>    0.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-15</th>                      <td>    0.1897</td> <td>    0.089</td> <td>    2.136</td> <td> 0.033</td> <td>    0.016</td> <td>    0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-16</th>                      <td>    0.0733</td> <td>    0.090</td> <td>    0.812</td> <td> 0.417</td> <td>   -0.104</td> <td>    0.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-17</th>                      <td>   -0.1827</td> <td>    0.110</td> <td>   -1.668</td> <td> 0.095</td> <td>   -0.397</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time-15-after</th>                        <td>    0.0983</td> <td>    0.021</td> <td>    4.684</td> <td> 0.000</td> <td>    0.057</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time-10-12</th>                           <td>    0.1549</td> <td>    0.022</td> <td>    7.180</td> <td> 0.000</td> <td>    0.113</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place_of_birth_jockey-outside_tokyo</th>  <td>   -0.0166</td> <td>    0.024</td> <td>   -0.692</td> <td> 0.489</td> <td>   -0.064</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place_of_birth_trainer-outside_tokyo</th> <td>   -0.0148</td> <td>    0.020</td> <td>   -0.734</td> <td> 0.463</td> <td>   -0.054</td> <td>    0.025</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>102675.695</td> <th>  Durbin-Watson:     </th>   <td>   1.962</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>226979364.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 5.268</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td>253.554</td>  <th>  Cond. No.          </th>   <td>1.00e+16</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          run_time_1000   R-squared:                       0.426\n",
       "Model:                            OLS   Adj. R-squared:                  0.425\n",
       "Method:                 Least Squares   F-statistic:                     971.6\n",
       "Date:                Mon, 12 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        20:35:56   Log-Likelihood:            -1.7226e+05\n",
       "No. Observations:               86622   AIC:                         3.446e+05\n",
       "Df Residuals:                   86555   BIC:                         3.453e+05\n",
       "Df Model:                          66                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "========================================================================================================\n",
       "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "const                                   60.2242      0.633     95.151      0.000      58.984      61.465\n",
       "jockey_weight                            0.0838      0.007     12.578      0.000       0.071       0.097\n",
       "win_odds                                 0.0003      0.000      2.471      0.013    5.54e-05       0.000\n",
       "win_fav                                  0.1028      0.002     45.776      0.000       0.098       0.107\n",
       "horse_weight                             0.0074      0.000     33.384      0.000       0.007       0.008\n",
       "curr_age                                -0.2335      0.013    -17.429      0.000      -0.260      -0.207\n",
       "horse_weight_increase                   -0.0534      0.002    -25.199      0.000      -0.058      -0.049\n",
       "jockey_age                              -0.0042      0.001     -4.859      0.000      -0.006      -0.002\n",
       "trainer_age                             -0.0024      0.001     -3.542      0.000      -0.004      -0.001\n",
       "place-中山                                 0.9802      0.037     26.680      0.000       0.908       1.052\n",
       "place-京都                                 0.2527      0.036      6.929      0.000       0.181       0.324\n",
       "place-東京                                 0.1641      0.261      0.628      0.530      -0.348       0.676\n",
       "place-阪神                                 0.3444      0.037      9.277      0.000       0.272       0.417\n",
       "place-新潟                                -0.1590      0.262     -0.606      0.545      -0.673       0.355\n",
       "place-小倉                                -0.3745      0.040     -9.347      0.000      -0.453      -0.296\n",
       "place-福島                                 0.2976      0.041      7.310      0.000       0.218       0.377\n",
       "place-中京                                 0.0286      0.263      0.109      0.913      -0.486       0.544\n",
       "place-札幌                                 0.3523      0.044      8.007      0.000       0.266       0.439\n",
       "type-芝                                  -5.5165      0.100    -54.938      0.000      -5.713      -5.320\n",
       "type-ダ                                  -3.0927      0.100    -30.953      0.000      -3.289      -2.897\n",
       "track-右                                 -2.1478      0.135    -15.903      0.000      -2.413      -1.883\n",
       "track-左                                 -1.7711      0.143    -12.428      0.000      -2.050      -1.492\n",
       "track-直                                 -4.6903      0.160    -29.390      0.000      -5.003      -4.377\n",
       "weather-晴                               -1.0084      0.203     -4.959      0.000      -1.407      -0.610\n",
       "weather-曇                               -1.0341      0.204     -5.081      0.000      -1.433      -0.635\n",
       "weather-雨                               -0.5828      0.205     -2.842      0.004      -0.985      -0.181\n",
       "weather-小雨                              -0.8757      0.206     -4.259      0.000      -1.279      -0.473\n",
       "weather-雪                                0.0611      0.280      0.218      0.827      -0.488       0.610\n",
       "condition-良                             -0.0470      0.033     -1.426      0.154      -0.112       0.018\n",
       "condition-稍重                             0.1928      0.035      5.508      0.000       0.124       0.261\n",
       "condition-重                             -0.0372      0.038     -0.971      0.332      -0.112       0.038\n",
       "gender-牡                                -0.0481      0.030     -1.588      0.112      -0.107       0.011\n",
       "gender-牝                                -0.2112      0.032     -6.628      0.000      -0.274      -0.149\n",
       "breed-鹿毛                                -0.6932      0.417     -1.662      0.097      -1.511       0.124\n",
       "breed-栗毛                                -0.7253      0.417     -1.738      0.082      -1.543       0.093\n",
       "breed-黒鹿毛                               -0.6463      0.417     -1.549      0.121      -1.464       0.172\n",
       "breed-芦毛                                -0.7116      0.418     -1.703      0.089      -1.531       0.107\n",
       "breed-青鹿毛                               -0.5769      0.418     -1.380      0.167      -1.396       0.242\n",
       "breed-青毛                                -0.5803      0.421     -1.380      0.168      -1.404       0.244\n",
       "breed-栃栗毛                               -0.5953      0.426     -1.399      0.162      -1.429       0.239\n",
       "bracket-8                                1.9782      0.071     27.874      0.000       1.839       2.117\n",
       "bracket-7                                1.7362      0.069     25.326      0.000       1.602       1.871\n",
       "bracket-6                                1.5205      0.066     22.958      0.000       1.391       1.650\n",
       "bracket-5                                1.3159      0.064     20.485      0.000       1.190       1.442\n",
       "bracket-4                                1.0767      0.061     17.756      0.000       0.958       1.196\n",
       "bracket-3                                0.8095      0.057     14.098      0.000       0.697       0.922\n",
       "bracket-2                                0.4183      0.046      9.126      0.000       0.328       0.508\n",
       "horse_number-6                           1.1650      0.092     12.636      0.000       0.984       1.346\n",
       "horse_number-2                           2.0629      0.103     19.947      0.000       1.860       2.266\n",
       "horse_number-4                           1.5230      0.095     15.958      0.000       1.336       1.710\n",
       "horse_number-3                           1.7055      0.098     17.491      0.000       1.514       1.897\n",
       "horse_number-1                           2.3276      0.112     20.699      0.000       2.107       2.548\n",
       "horse_number-5                           1.2813      0.093     13.742      0.000       1.099       1.464\n",
       "horse_number-8                           0.9185      0.090     10.187      0.000       0.742       1.095\n",
       "horse_number-7                           1.0087      0.091     11.074      0.000       0.830       1.187\n",
       "horse_number-9                           0.7936      0.090      8.858      0.000       0.618       0.969\n",
       "horse_number-10                          0.6795      0.089      7.612      0.000       0.505       0.854\n",
       "horse_number-11                          0.5423      0.089      6.085      0.000       0.368       0.717\n",
       "horse_number-12                          0.4920      0.089      5.520      0.000       0.317       0.667\n",
       "horse_number-13                          0.4292      0.089      4.802      0.000       0.254       0.604\n",
       "horse_number-14                          0.3364      0.090      3.753      0.000       0.161       0.512\n",
       "horse_number-15                          0.1897      0.089      2.136      0.033       0.016       0.364\n",
       "horse_number-16                          0.0733      0.090      0.812      0.417      -0.104       0.250\n",
       "horse_number-17                         -0.1827      0.110     -1.668      0.095      -0.397       0.032\n",
       "time-15-after                            0.0983      0.021      4.684      0.000       0.057       0.139\n",
       "time-10-12                               0.1549      0.022      7.180      0.000       0.113       0.197\n",
       "place_of_birth_jockey-outside_tokyo     -0.0166      0.024     -0.692      0.489      -0.064       0.030\n",
       "place_of_birth_trainer-outside_tokyo    -0.0148      0.020     -0.734      0.463      -0.054       0.025\n",
       "==============================================================================\n",
       "Omnibus:                   102675.695   Durbin-Watson:                   1.962\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        226979364.451\n",
       "Skew:                           5.268   Prob(JB):                         0.00\n",
       "Kurtosis:                     253.554   Cond. No.                     1.00e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.97e-22. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_first = feature_engineer(first_occur_df)\n",
    "X_first = new_df_first.loc[:, new_df_first.columns != 'run_time_1000']\n",
    "y_first = new_df_first.loc[:, 'run_time_1000']\n",
    "X_first = sm.add_constant(X_first)\n",
    "results = sm.OLS(y_first, X_first).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 OLS for Full Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>run_time_1000</td>  <th>  R-squared:         </th>  <td>   0.557</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.557</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.635e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:36:52</td>     <th>  Log-Likelihood:    </th> <td>-1.6583e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>857855</td>      <th>  AIC:               </th>  <td>3.317e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>857788</td>      <th>  BIC:               </th>  <td>3.317e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    66</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                    <td></td>                      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                                <td>   44.6347</td> <td>    0.142</td> <td>  314.697</td> <td> 0.000</td> <td>   44.357</td> <td>   44.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jockey_weight</th>                        <td>    0.0207</td> <td>    0.002</td> <td>   13.127</td> <td> 0.000</td> <td>    0.018</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>win_odds</th>                             <td>    0.0015</td> <td> 2.91e-05</td> <td>   51.592</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>win_fav</th>                              <td>    0.0670</td> <td>    0.001</td> <td>  106.068</td> <td> 0.000</td> <td>    0.066</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_weight</th>                         <td>    0.0019</td> <td> 7.04e-05</td> <td>   26.474</td> <td> 0.000</td> <td>    0.002</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>curr_age</th>                             <td>   -0.3122</td> <td>    0.002</td> <td> -174.727</td> <td> 0.000</td> <td>   -0.316</td> <td>   -0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_weight_increase</th>                <td>   -0.0095</td> <td>    0.000</td> <td>  -24.545</td> <td> 0.000</td> <td>   -0.010</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jockey_age</th>                           <td>   -0.0038</td> <td>    0.000</td> <td>  -15.223</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trainer_age</th>                          <td>   -0.0036</td> <td>    0.000</td> <td>  -17.275</td> <td> 0.000</td> <td>   -0.004</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-東京</th>                             <td>   -0.7808</td> <td>    0.023</td> <td>  -33.403</td> <td> 0.000</td> <td>   -0.827</td> <td>   -0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-京都</th>                             <td>   -0.2908</td> <td>    0.010</td> <td>  -29.075</td> <td> 0.000</td> <td>   -0.310</td> <td>   -0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-中山</th>                             <td>    0.3292</td> <td>    0.010</td> <td>   32.565</td> <td> 0.000</td> <td>    0.309</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-阪神</th>                             <td>   -0.1360</td> <td>    0.010</td> <td>  -13.467</td> <td> 0.000</td> <td>   -0.156</td> <td>   -0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-新潟</th>                             <td>   -0.8478</td> <td>    0.024</td> <td>  -35.838</td> <td> 0.000</td> <td>   -0.894</td> <td>   -0.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-中京</th>                             <td>   -0.4896</td> <td>    0.024</td> <td>  -20.548</td> <td> 0.000</td> <td>   -0.536</td> <td>   -0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-小倉</th>                             <td>   -0.4753</td> <td>    0.011</td> <td>  -44.243</td> <td> 0.000</td> <td>   -0.496</td> <td>   -0.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-福島</th>                             <td>    0.0118</td> <td>    0.011</td> <td>    1.089</td> <td> 0.276</td> <td>   -0.009</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-札幌</th>                             <td>   -0.0427</td> <td>    0.012</td> <td>   -3.520</td> <td> 0.000</td> <td>   -0.066</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type-ダ</th>                               <td>   12.8745</td> <td>    0.048</td> <td>  267.683</td> <td> 0.000</td> <td>   12.780</td> <td>   12.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type-芝</th>                               <td>   10.5935</td> <td>    0.048</td> <td>  221.138</td> <td> 0.000</td> <td>   10.500</td> <td>   10.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-右</th>                              <td>    2.7598</td> <td>    0.030</td> <td>   93.284</td> <td> 0.000</td> <td>    2.702</td> <td>    2.818</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-左</th>                              <td>    3.4127</td> <td>    0.021</td> <td>  164.245</td> <td> 0.000</td> <td>    3.372</td> <td>    3.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-芝</th>                              <td>   21.1667</td> <td>    0.051</td> <td>  413.546</td> <td> 0.000</td> <td>   21.066</td> <td>   21.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-晴</th>                            <td>   -1.0103</td> <td>    0.064</td> <td>  -15.803</td> <td> 0.000</td> <td>   -1.136</td> <td>   -0.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-曇</th>                            <td>   -1.0103</td> <td>    0.064</td> <td>  -15.792</td> <td> 0.000</td> <td>   -1.136</td> <td>   -0.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-雨</th>                            <td>   -0.6344</td> <td>    0.064</td> <td>   -9.857</td> <td> 0.000</td> <td>   -0.761</td> <td>   -0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-小雨</th>                           <td>   -0.8462</td> <td>    0.065</td> <td>  -13.106</td> <td> 0.000</td> <td>   -0.973</td> <td>   -0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-小雪</th>                           <td>   -0.7106</td> <td>    0.085</td> <td>   -8.325</td> <td> 0.000</td> <td>   -0.878</td> <td>   -0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-良</th>                          <td>    0.0233</td> <td>    0.010</td> <td>    2.449</td> <td> 0.014</td> <td>    0.005</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-稍重</th>                         <td>    0.1485</td> <td>    0.010</td> <td>   14.634</td> <td> 0.000</td> <td>    0.129</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-重</th>                          <td>   -0.0279</td> <td>    0.011</td> <td>   -2.517</td> <td> 0.012</td> <td>   -0.050</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender-牡</th>                             <td>   -0.0308</td> <td>    0.008</td> <td>   -3.903</td> <td> 0.000</td> <td>   -0.046</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender-牝</th>                             <td>   -0.3090</td> <td>    0.009</td> <td>  -35.566</td> <td> 0.000</td> <td>   -0.326</td> <td>   -0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-鹿毛</th>                             <td>   -0.5823</td> <td>    0.145</td> <td>   -4.014</td> <td> 0.000</td> <td>   -0.867</td> <td>   -0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-栗毛</th>                             <td>   -0.6439</td> <td>    0.145</td> <td>   -4.439</td> <td> 0.000</td> <td>   -0.928</td> <td>   -0.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-黒鹿毛</th>                            <td>   -0.5456</td> <td>    0.145</td> <td>   -3.761</td> <td> 0.000</td> <td>   -0.830</td> <td>   -0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-芦毛</th>                             <td>   -0.6067</td> <td>    0.145</td> <td>   -4.178</td> <td> 0.000</td> <td>   -0.891</td> <td>   -0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-青鹿毛</th>                            <td>   -0.5601</td> <td>    0.145</td> <td>   -3.857</td> <td> 0.000</td> <td>   -0.845</td> <td>   -0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-青毛</th>                             <td>   -0.5174</td> <td>    0.146</td> <td>   -3.547</td> <td> 0.000</td> <td>   -0.803</td> <td>   -0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-栃栗毛</th>                            <td>   -0.5162</td> <td>    0.147</td> <td>   -3.509</td> <td> 0.000</td> <td>   -0.805</td> <td>   -0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-8</th>                            <td>    2.7124</td> <td>    0.022</td> <td>  125.561</td> <td> 0.000</td> <td>    2.670</td> <td>    2.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-7</th>                            <td>    2.3610</td> <td>    0.021</td> <td>  113.473</td> <td> 0.000</td> <td>    2.320</td> <td>    2.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-6</th>                            <td>    2.0783</td> <td>    0.020</td> <td>  103.424</td> <td> 0.000</td> <td>    2.039</td> <td>    2.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-5</th>                            <td>    1.7915</td> <td>    0.019</td> <td>   92.228</td> <td> 0.000</td> <td>    1.753</td> <td>    1.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-4</th>                            <td>    1.4826</td> <td>    0.018</td> <td>   80.804</td> <td> 0.000</td> <td>    1.447</td> <td>    1.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-3</th>                            <td>    1.1323</td> <td>    0.017</td> <td>   65.581</td> <td> 0.000</td> <td>    1.098</td> <td>    1.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-2</th>                            <td>    0.6286</td> <td>    0.014</td> <td>   45.541</td> <td> 0.000</td> <td>    0.602</td> <td>    0.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-2</th>                       <td>    2.9100</td> <td>    0.030</td> <td>   95.431</td> <td> 0.000</td> <td>    2.850</td> <td>    2.970</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-4</th>                       <td>    2.2289</td> <td>    0.028</td> <td>   79.569</td> <td> 0.000</td> <td>    2.174</td> <td>    2.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-5</th>                       <td>    1.9014</td> <td>    0.027</td> <td>   69.636</td> <td> 0.000</td> <td>    1.848</td> <td>    1.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-3</th>                       <td>    2.4108</td> <td>    0.029</td> <td>   84.183</td> <td> 0.000</td> <td>    2.355</td> <td>    2.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-1</th>                       <td>    3.2801</td> <td>    0.033</td> <td>   99.152</td> <td> 0.000</td> <td>    3.215</td> <td>    3.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-6</th>                       <td>    1.7836</td> <td>    0.027</td> <td>   66.110</td> <td> 0.000</td> <td>    1.731</td> <td>    1.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-7</th>                       <td>    1.5522</td> <td>    0.027</td> <td>   58.372</td> <td> 0.000</td> <td>    1.500</td> <td>    1.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-8</th>                       <td>    1.4564</td> <td>    0.026</td> <td>   55.328</td> <td> 0.000</td> <td>    1.405</td> <td>    1.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-9</th>                       <td>    1.2539</td> <td>    0.026</td> <td>   48.090</td> <td> 0.000</td> <td>    1.203</td> <td>    1.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-10</th>                      <td>    1.1680</td> <td>    0.026</td> <td>   45.037</td> <td> 0.000</td> <td>    1.117</td> <td>    1.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-11</th>                      <td>    0.9683</td> <td>    0.026</td> <td>   37.527</td> <td> 0.000</td> <td>    0.918</td> <td>    1.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-12</th>                      <td>    0.8930</td> <td>    0.026</td> <td>   34.718</td> <td> 0.000</td> <td>    0.843</td> <td>    0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-13</th>                      <td>    0.7283</td> <td>    0.026</td> <td>   28.322</td> <td> 0.000</td> <td>    0.678</td> <td>    0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-14</th>                      <td>    0.6202</td> <td>    0.026</td> <td>   24.097</td> <td> 0.000</td> <td>    0.570</td> <td>    0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-15</th>                      <td>    0.3882</td> <td>    0.025</td> <td>   15.267</td> <td> 0.000</td> <td>    0.338</td> <td>    0.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-16</th>                      <td>    0.2462</td> <td>    0.026</td> <td>    9.548</td> <td> 0.000</td> <td>    0.196</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-17</th>                      <td>    0.0010</td> <td>    0.032</td> <td>    0.031</td> <td> 0.976</td> <td>   -0.062</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time-10-12</th>                           <td>    0.4321</td> <td>    0.005</td> <td>   85.236</td> <td> 0.000</td> <td>    0.422</td> <td>    0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time-15-after</th>                        <td>   -0.1514</td> <td>    0.005</td> <td>  -33.143</td> <td> 0.000</td> <td>   -0.160</td> <td>   -0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place_of_birth_jockey-outside_tokyo</th>  <td>   -0.0692</td> <td>    0.007</td> <td>   -9.847</td> <td> 0.000</td> <td>   -0.083</td> <td>   -0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place_of_birth_trainer-outside_tokyo</th> <td>   -0.0490</td> <td>    0.006</td> <td>   -8.193</td> <td> 0.000</td> <td>   -0.061</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>379363.098</td> <th>  Durbin-Watson:     </th>   <td>   1.185</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>50886002.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 1.111</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td>40.666</td>   <th>  Cond. No.          </th>   <td>3.90e+15</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          run_time_1000   R-squared:                       0.557\n",
       "Model:                            OLS   Adj. R-squared:                  0.557\n",
       "Method:                 Least Squares   F-statistic:                 1.635e+04\n",
       "Date:                Mon, 12 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        20:36:52   Log-Likelihood:            -1.6583e+06\n",
       "No. Observations:              857855   AIC:                         3.317e+06\n",
       "Df Residuals:                  857788   BIC:                         3.317e+06\n",
       "Df Model:                          66                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "========================================================================================================\n",
       "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "const                                   44.6347      0.142    314.697      0.000      44.357      44.913\n",
       "jockey_weight                            0.0207      0.002     13.127      0.000       0.018       0.024\n",
       "win_odds                                 0.0015   2.91e-05     51.592      0.000       0.001       0.002\n",
       "win_fav                                  0.0670      0.001    106.068      0.000       0.066       0.068\n",
       "horse_weight                             0.0019   7.04e-05     26.474      0.000       0.002       0.002\n",
       "curr_age                                -0.3122      0.002   -174.727      0.000      -0.316      -0.309\n",
       "horse_weight_increase                   -0.0095      0.000    -24.545      0.000      -0.010      -0.009\n",
       "jockey_age                              -0.0038      0.000    -15.223      0.000      -0.004      -0.003\n",
       "trainer_age                             -0.0036      0.000    -17.275      0.000      -0.004      -0.003\n",
       "place-東京                                -0.7808      0.023    -33.403      0.000      -0.827      -0.735\n",
       "place-京都                                -0.2908      0.010    -29.075      0.000      -0.310      -0.271\n",
       "place-中山                                 0.3292      0.010     32.565      0.000       0.309       0.349\n",
       "place-阪神                                -0.1360      0.010    -13.467      0.000      -0.156      -0.116\n",
       "place-新潟                                -0.8478      0.024    -35.838      0.000      -0.894      -0.801\n",
       "place-中京                                -0.4896      0.024    -20.548      0.000      -0.536      -0.443\n",
       "place-小倉                                -0.4753      0.011    -44.243      0.000      -0.496      -0.454\n",
       "place-福島                                 0.0118      0.011      1.089      0.276      -0.009       0.033\n",
       "place-札幌                                -0.0427      0.012     -3.520      0.000      -0.066      -0.019\n",
       "type-ダ                                  12.8745      0.048    267.683      0.000      12.780      12.969\n",
       "type-芝                                  10.5935      0.048    221.138      0.000      10.500      10.687\n",
       "track-右                                  2.7598      0.030     93.284      0.000       2.702       2.818\n",
       "track-左                                  3.4127      0.021    164.245      0.000       3.372       3.453\n",
       "track-芝                                 21.1667      0.051    413.546      0.000      21.066      21.267\n",
       "weather-晴                               -1.0103      0.064    -15.803      0.000      -1.136      -0.885\n",
       "weather-曇                               -1.0103      0.064    -15.792      0.000      -1.136      -0.885\n",
       "weather-雨                               -0.6344      0.064     -9.857      0.000      -0.761      -0.508\n",
       "weather-小雨                              -0.8462      0.065    -13.106      0.000      -0.973      -0.720\n",
       "weather-小雪                              -0.7106      0.085     -8.325      0.000      -0.878      -0.543\n",
       "condition-良                              0.0233      0.010      2.449      0.014       0.005       0.042\n",
       "condition-稍重                             0.1485      0.010     14.634      0.000       0.129       0.168\n",
       "condition-重                             -0.0279      0.011     -2.517      0.012      -0.050      -0.006\n",
       "gender-牡                                -0.0308      0.008     -3.903      0.000      -0.046      -0.015\n",
       "gender-牝                                -0.3090      0.009    -35.566      0.000      -0.326      -0.292\n",
       "breed-鹿毛                                -0.5823      0.145     -4.014      0.000      -0.867      -0.298\n",
       "breed-栗毛                                -0.6439      0.145     -4.439      0.000      -0.928      -0.360\n",
       "breed-黒鹿毛                               -0.5456      0.145     -3.761      0.000      -0.830      -0.261\n",
       "breed-芦毛                                -0.6067      0.145     -4.178      0.000      -0.891      -0.322\n",
       "breed-青鹿毛                               -0.5601      0.145     -3.857      0.000      -0.845      -0.276\n",
       "breed-青毛                                -0.5174      0.146     -3.547      0.000      -0.803      -0.232\n",
       "breed-栃栗毛                               -0.5162      0.147     -3.509      0.000      -0.805      -0.228\n",
       "bracket-8                                2.7124      0.022    125.561      0.000       2.670       2.755\n",
       "bracket-7                                2.3610      0.021    113.473      0.000       2.320       2.402\n",
       "bracket-6                                2.0783      0.020    103.424      0.000       2.039       2.118\n",
       "bracket-5                                1.7915      0.019     92.228      0.000       1.753       1.830\n",
       "bracket-4                                1.4826      0.018     80.804      0.000       1.447       1.519\n",
       "bracket-3                                1.1323      0.017     65.581      0.000       1.098       1.166\n",
       "bracket-2                                0.6286      0.014     45.541      0.000       0.602       0.656\n",
       "horse_number-2                           2.9100      0.030     95.431      0.000       2.850       2.970\n",
       "horse_number-4                           2.2289      0.028     79.569      0.000       2.174       2.284\n",
       "horse_number-5                           1.9014      0.027     69.636      0.000       1.848       1.955\n",
       "horse_number-3                           2.4108      0.029     84.183      0.000       2.355       2.467\n",
       "horse_number-1                           3.2801      0.033     99.152      0.000       3.215       3.345\n",
       "horse_number-6                           1.7836      0.027     66.110      0.000       1.731       1.836\n",
       "horse_number-7                           1.5522      0.027     58.372      0.000       1.500       1.604\n",
       "horse_number-8                           1.4564      0.026     55.328      0.000       1.405       1.508\n",
       "horse_number-9                           1.2539      0.026     48.090      0.000       1.203       1.305\n",
       "horse_number-10                          1.1680      0.026     45.037      0.000       1.117       1.219\n",
       "horse_number-11                          0.9683      0.026     37.527      0.000       0.918       1.019\n",
       "horse_number-12                          0.8930      0.026     34.718      0.000       0.843       0.943\n",
       "horse_number-13                          0.7283      0.026     28.322      0.000       0.678       0.779\n",
       "horse_number-14                          0.6202      0.026     24.097      0.000       0.570       0.671\n",
       "horse_number-15                          0.3882      0.025     15.267      0.000       0.338       0.438\n",
       "horse_number-16                          0.2462      0.026      9.548      0.000       0.196       0.297\n",
       "horse_number-17                          0.0010      0.032      0.031      0.976      -0.062       0.064\n",
       "time-10-12                               0.4321      0.005     85.236      0.000       0.422       0.442\n",
       "time-15-after                           -0.1514      0.005    -33.143      0.000      -0.160      -0.142\n",
       "place_of_birth_jockey-outside_tokyo     -0.0692      0.007     -9.847      0.000      -0.083      -0.055\n",
       "place_of_birth_trainer-outside_tokyo    -0.0490      0.006     -8.193      0.000      -0.061      -0.037\n",
       "==============================================================================\n",
       "Omnibus:                   379363.098   Durbin-Watson:                   1.185\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         50886002.269\n",
       "Skew:                           1.111   Prob(JB):                         0.00\n",
       "Kurtosis:                      40.666   Cond. No.                     3.90e+15\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.31e-20. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_full = feature_engineer(horse_race_df)\n",
    "X_full = new_df_full.loc[:, new_df_full.columns != 'run_time_1000']\n",
    "y_full = new_df_full.loc[:, 'run_time_1000']\n",
    "X_full = sm.add_constant(X_full)\n",
    "results = sm.OLS(y_full, X_full).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>run_time_1000</td>  <th>  R-squared:         </th>  <td>   0.640</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.640</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>2.045e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:39:07</td>     <th>  Log-Likelihood:    </th> <td>-1.4146e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>771242</td>      <th>  AIC:               </th>  <td>2.829e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>771174</td>      <th>  BIC:               </th>  <td>2.830e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    67</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                    <td></td>                      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                                <td>   30.5710</td> <td>    0.142</td> <td>  215.162</td> <td> 0.000</td> <td>   30.293</td> <td>   30.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jockey_weight</th>                        <td>    0.0078</td> <td>    0.001</td> <td>    5.235</td> <td> 0.000</td> <td>    0.005</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>win_odds</th>                             <td>    0.0009</td> <td> 2.76e-05</td> <td>   33.620</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>win_fav</th>                              <td>    0.0360</td> <td>    0.001</td> <td>   59.542</td> <td> 0.000</td> <td>    0.035</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_weight</th>                         <td>   -0.0004</td> <td> 6.79e-05</td> <td>   -5.214</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>curr_age</th>                             <td>   -0.1599</td> <td>    0.002</td> <td>  -91.514</td> <td> 0.000</td> <td>   -0.163</td> <td>   -0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_weight_increase</th>                <td>    0.0021</td> <td>    0.000</td> <td>    5.620</td> <td> 0.000</td> <td>    0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jockey_age</th>                           <td>   -0.0026</td> <td>    0.000</td> <td>  -10.811</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trainer_age</th>                          <td>   -0.0015</td> <td>    0.000</td> <td>   -7.381</td> <td> 0.000</td> <td>   -0.002</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-東京</th>                             <td>   -0.6830</td> <td>    0.021</td> <td>  -31.893</td> <td> 0.000</td> <td>   -0.725</td> <td>   -0.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-京都</th>                             <td>   -0.2069</td> <td>    0.009</td> <td>  -21.801</td> <td> 0.000</td> <td>   -0.225</td> <td>   -0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-中山</th>                             <td>    0.2919</td> <td>    0.010</td> <td>   30.451</td> <td> 0.000</td> <td>    0.273</td> <td>    0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-阪神</th>                             <td>   -0.0586</td> <td>    0.010</td> <td>   -6.127</td> <td> 0.000</td> <td>   -0.077</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-新潟</th>                             <td>   -0.7278</td> <td>    0.022</td> <td>  -33.568</td> <td> 0.000</td> <td>   -0.770</td> <td>   -0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-中京</th>                             <td>   -0.3550</td> <td>    0.022</td> <td>  -16.258</td> <td> 0.000</td> <td>   -0.398</td> <td>   -0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-小倉</th>                             <td>   -0.3762</td> <td>    0.010</td> <td>  -37.063</td> <td> 0.000</td> <td>   -0.396</td> <td>   -0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-福島</th>                             <td>   -0.0144</td> <td>    0.010</td> <td>   -1.402</td> <td> 0.161</td> <td>   -0.035</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place-函館</th>                             <td>    0.1050</td> <td>    0.011</td> <td>    9.138</td> <td> 0.000</td> <td>    0.082</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type-ダ</th>                               <td>    8.6148</td> <td>    0.048</td> <td>  180.432</td> <td> 0.000</td> <td>    8.521</td> <td>    8.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type-芝</th>                               <td>    6.7773</td> <td>    0.047</td> <td>  143.318</td> <td> 0.000</td> <td>    6.685</td> <td>    6.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-右</th>                              <td>    2.4738</td> <td>    0.027</td> <td>   90.739</td> <td> 0.000</td> <td>    2.420</td> <td>    2.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-左</th>                              <td>    3.0292</td> <td>    0.019</td> <td>  155.944</td> <td> 0.000</td> <td>    2.991</td> <td>    3.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track-芝</th>                              <td>   15.1789</td> <td>    0.052</td> <td>  293.239</td> <td> 0.000</td> <td>   15.077</td> <td>   15.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-晴</th>                            <td>   -0.8307</td> <td>    0.062</td> <td>  -13.435</td> <td> 0.000</td> <td>   -0.952</td> <td>   -0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-曇</th>                            <td>   -0.8228</td> <td>    0.062</td> <td>  -13.299</td> <td> 0.000</td> <td>   -0.944</td> <td>   -0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-雨</th>                            <td>   -0.4431</td> <td>    0.062</td> <td>   -7.120</td> <td> 0.000</td> <td>   -0.565</td> <td>   -0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-小雨</th>                           <td>   -0.6764</td> <td>    0.062</td> <td>  -10.836</td> <td> 0.000</td> <td>   -0.799</td> <td>   -0.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weather-小雪</th>                           <td>   -0.6202</td> <td>    0.082</td> <td>   -7.578</td> <td> 0.000</td> <td>   -0.781</td> <td>   -0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-良</th>                          <td>    0.0612</td> <td>    0.009</td> <td>    6.741</td> <td> 0.000</td> <td>    0.043</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-稍重</th>                         <td>    0.1592</td> <td>    0.010</td> <td>   16.480</td> <td> 0.000</td> <td>    0.140</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition-重</th>                          <td>   -0.0156</td> <td>    0.011</td> <td>   -1.482</td> <td> 0.138</td> <td>   -0.036</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender-牡</th>                             <td>   -0.0050</td> <td>    0.007</td> <td>   -0.671</td> <td> 0.503</td> <td>   -0.020</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender-牝</th>                             <td>   -0.1918</td> <td>    0.008</td> <td>  -23.262</td> <td> 0.000</td> <td>   -0.208</td> <td>   -0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-鹿毛</th>                             <td>   -0.2378</td> <td>    0.141</td> <td>   -1.683</td> <td> 0.092</td> <td>   -0.515</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-栗毛</th>                             <td>   -0.2922</td> <td>    0.141</td> <td>   -2.068</td> <td> 0.039</td> <td>   -0.569</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-黒鹿毛</th>                            <td>   -0.2089</td> <td>    0.141</td> <td>   -1.478</td> <td> 0.139</td> <td>   -0.486</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-芦毛</th>                             <td>   -0.2637</td> <td>    0.141</td> <td>   -1.864</td> <td> 0.062</td> <td>   -0.541</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-青鹿毛</th>                            <td>   -0.2222</td> <td>    0.141</td> <td>   -1.571</td> <td> 0.116</td> <td>   -0.500</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-青毛</th>                             <td>   -0.1807</td> <td>    0.142</td> <td>   -1.272</td> <td> 0.204</td> <td>   -0.459</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>breed-栃栗毛</th>                            <td>   -0.2194</td> <td>    0.143</td> <td>   -1.532</td> <td> 0.126</td> <td>   -0.500</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-8</th>                            <td>    2.1413</td> <td>    0.021</td> <td>  102.880</td> <td> 0.000</td> <td>    2.101</td> <td>    2.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-7</th>                            <td>    1.8635</td> <td>    0.020</td> <td>   93.084</td> <td> 0.000</td> <td>    1.824</td> <td>    1.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-6</th>                            <td>    1.6427</td> <td>    0.019</td> <td>   85.043</td> <td> 0.000</td> <td>    1.605</td> <td>    1.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-5</th>                            <td>    1.4137</td> <td>    0.019</td> <td>   75.814</td> <td> 0.000</td> <td>    1.377</td> <td>    1.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-4</th>                            <td>    1.1724</td> <td>    0.018</td> <td>   66.613</td> <td> 0.000</td> <td>    1.138</td> <td>    1.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-3</th>                            <td>    0.9010</td> <td>    0.017</td> <td>   54.491</td> <td> 0.000</td> <td>    0.869</td> <td>    0.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bracket-2</th>                            <td>    0.5064</td> <td>    0.013</td> <td>   38.335</td> <td> 0.000</td> <td>    0.481</td> <td>    0.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-5</th>                       <td>    1.4926</td> <td>    0.026</td> <td>   57.212</td> <td> 0.000</td> <td>    1.441</td> <td>    1.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-1</th>                       <td>    2.5736</td> <td>    0.032</td> <td>   81.240</td> <td> 0.000</td> <td>    2.511</td> <td>    2.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-4</th>                       <td>    1.7447</td> <td>    0.027</td> <td>   65.126</td> <td> 0.000</td> <td>    1.692</td> <td>    1.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-2</th>                       <td>    2.2772</td> <td>    0.029</td> <td>   77.974</td> <td> 0.000</td> <td>    2.220</td> <td>    2.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-3</th>                       <td>    1.8845</td> <td>    0.027</td> <td>   68.788</td> <td> 0.000</td> <td>    1.831</td> <td>    1.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-7</th>                       <td>    1.2241</td> <td>    0.025</td> <td>   48.230</td> <td> 0.000</td> <td>    1.174</td> <td>    1.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-6</th>                       <td>    1.4018</td> <td>    0.026</td> <td>   54.395</td> <td> 0.000</td> <td>    1.351</td> <td>    1.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-8</th>                       <td>    1.1470</td> <td>    0.025</td> <td>   45.660</td> <td> 0.000</td> <td>    1.098</td> <td>    1.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-9</th>                       <td>    0.9919</td> <td>    0.025</td> <td>   39.892</td> <td> 0.000</td> <td>    0.943</td> <td>    1.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-10</th>                      <td>    0.9243</td> <td>    0.025</td> <td>   37.386</td> <td> 0.000</td> <td>    0.876</td> <td>    0.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-11</th>                      <td>    0.7739</td> <td>    0.025</td> <td>   31.487</td> <td> 0.000</td> <td>    0.726</td> <td>    0.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-12</th>                      <td>    0.7134</td> <td>    0.024</td> <td>   29.136</td> <td> 0.000</td> <td>    0.665</td> <td>    0.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-13</th>                      <td>    0.5883</td> <td>    0.024</td> <td>   24.045</td> <td> 0.000</td> <td>    0.540</td> <td>    0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-14</th>                      <td>    0.5067</td> <td>    0.024</td> <td>   20.699</td> <td> 0.000</td> <td>    0.459</td> <td>    0.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-15</th>                      <td>    0.3311</td> <td>    0.024</td> <td>   13.697</td> <td> 0.000</td> <td>    0.284</td> <td>    0.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-16</th>                      <td>    0.2038</td> <td>    0.025</td> <td>    8.313</td> <td> 0.000</td> <td>    0.156</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horse_number-17</th>                      <td>    0.0288</td> <td>    0.030</td> <td>    0.944</td> <td> 0.345</td> <td>   -0.031</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time-10-12</th>                           <td>    0.3385</td> <td>    0.005</td> <td>   69.792</td> <td> 0.000</td> <td>    0.329</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time-15-after</th>                        <td>   -0.1584</td> <td>    0.004</td> <td>  -36.383</td> <td> 0.000</td> <td>   -0.167</td> <td>   -0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place_of_birth_jockey-outside_tokyo</th>  <td>   -0.0580</td> <td>    0.007</td> <td>   -8.662</td> <td> 0.000</td> <td>   -0.071</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>place_of_birth_trainer-outside_tokyo</th> <td>   -0.0353</td> <td>    0.006</td> <td>   -6.183</td> <td> 0.000</td> <td>   -0.047</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>last_run_time</th>                        <td>    0.3181</td> <td>    0.001</td> <td>  365.981</td> <td> 0.000</td> <td>    0.316</td> <td>    0.320</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>189316.328</td> <th>  Durbin-Watson:     </th>  <td>   1.811</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>3523847.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 0.709</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td>13.375</td>   <th>  Cond. No.          </th>  <td>3.85e+15</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          run_time_1000   R-squared:                       0.640\n",
       "Model:                            OLS   Adj. R-squared:                  0.640\n",
       "Method:                 Least Squares   F-statistic:                 2.045e+04\n",
       "Date:                Mon, 12 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        20:39:07   Log-Likelihood:            -1.4146e+06\n",
       "No. Observations:              771242   AIC:                         2.829e+06\n",
       "Df Residuals:                  771174   BIC:                         2.830e+06\n",
       "Df Model:                          67                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "========================================================================================================\n",
       "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "const                                   30.5710      0.142    215.162      0.000      30.293      30.850\n",
       "jockey_weight                            0.0078      0.001      5.235      0.000       0.005       0.011\n",
       "win_odds                                 0.0009   2.76e-05     33.620      0.000       0.001       0.001\n",
       "win_fav                                  0.0360      0.001     59.542      0.000       0.035       0.037\n",
       "horse_weight                            -0.0004   6.79e-05     -5.214      0.000      -0.000      -0.000\n",
       "curr_age                                -0.1599      0.002    -91.514      0.000      -0.163      -0.156\n",
       "horse_weight_increase                    0.0021      0.000      5.620      0.000       0.001       0.003\n",
       "jockey_age                              -0.0026      0.000    -10.811      0.000      -0.003      -0.002\n",
       "trainer_age                             -0.0015      0.000     -7.381      0.000      -0.002      -0.001\n",
       "place-東京                                -0.6830      0.021    -31.893      0.000      -0.725      -0.641\n",
       "place-京都                                -0.2069      0.009    -21.801      0.000      -0.225      -0.188\n",
       "place-中山                                 0.2919      0.010     30.451      0.000       0.273       0.311\n",
       "place-阪神                                -0.0586      0.010     -6.127      0.000      -0.077      -0.040\n",
       "place-新潟                                -0.7278      0.022    -33.568      0.000      -0.770      -0.685\n",
       "place-中京                                -0.3550      0.022    -16.258      0.000      -0.398      -0.312\n",
       "place-小倉                                -0.3762      0.010    -37.063      0.000      -0.396      -0.356\n",
       "place-福島                                -0.0144      0.010     -1.402      0.161      -0.035       0.006\n",
       "place-函館                                 0.1050      0.011      9.138      0.000       0.082       0.127\n",
       "type-ダ                                   8.6148      0.048    180.432      0.000       8.521       8.708\n",
       "type-芝                                   6.7773      0.047    143.318      0.000       6.685       6.870\n",
       "track-右                                  2.4738      0.027     90.739      0.000       2.420       2.527\n",
       "track-左                                  3.0292      0.019    155.944      0.000       2.991       3.067\n",
       "track-芝                                 15.1789      0.052    293.239      0.000      15.077      15.280\n",
       "weather-晴                               -0.8307      0.062    -13.435      0.000      -0.952      -0.710\n",
       "weather-曇                               -0.8228      0.062    -13.299      0.000      -0.944      -0.702\n",
       "weather-雨                               -0.4431      0.062     -7.120      0.000      -0.565      -0.321\n",
       "weather-小雨                              -0.6764      0.062    -10.836      0.000      -0.799      -0.554\n",
       "weather-小雪                              -0.6202      0.082     -7.578      0.000      -0.781      -0.460\n",
       "condition-良                              0.0612      0.009      6.741      0.000       0.043       0.079\n",
       "condition-稍重                             0.1592      0.010     16.480      0.000       0.140       0.178\n",
       "condition-重                             -0.0156      0.011     -1.482      0.138      -0.036       0.005\n",
       "gender-牡                                -0.0050      0.007     -0.671      0.503      -0.020       0.010\n",
       "gender-牝                                -0.1918      0.008    -23.262      0.000      -0.208      -0.176\n",
       "breed-鹿毛                                -0.2378      0.141     -1.683      0.092      -0.515       0.039\n",
       "breed-栗毛                                -0.2922      0.141     -2.068      0.039      -0.569      -0.015\n",
       "breed-黒鹿毛                               -0.2089      0.141     -1.478      0.139      -0.486       0.068\n",
       "breed-芦毛                                -0.2637      0.141     -1.864      0.062      -0.541       0.014\n",
       "breed-青鹿毛                               -0.2222      0.141     -1.571      0.116      -0.500       0.055\n",
       "breed-青毛                                -0.1807      0.142     -1.272      0.204      -0.459       0.098\n",
       "breed-栃栗毛                               -0.2194      0.143     -1.532      0.126      -0.500       0.061\n",
       "bracket-8                                2.1413      0.021    102.880      0.000       2.101       2.182\n",
       "bracket-7                                1.8635      0.020     93.084      0.000       1.824       1.903\n",
       "bracket-6                                1.6427      0.019     85.043      0.000       1.605       1.681\n",
       "bracket-5                                1.4137      0.019     75.814      0.000       1.377       1.450\n",
       "bracket-4                                1.1724      0.018     66.613      0.000       1.138       1.207\n",
       "bracket-3                                0.9010      0.017     54.491      0.000       0.869       0.933\n",
       "bracket-2                                0.5064      0.013     38.335      0.000       0.481       0.532\n",
       "horse_number-5                           1.4926      0.026     57.212      0.000       1.441       1.544\n",
       "horse_number-1                           2.5736      0.032     81.240      0.000       2.511       2.636\n",
       "horse_number-4                           1.7447      0.027     65.126      0.000       1.692       1.797\n",
       "horse_number-2                           2.2772      0.029     77.974      0.000       2.220       2.334\n",
       "horse_number-3                           1.8845      0.027     68.788      0.000       1.831       1.938\n",
       "horse_number-7                           1.2241      0.025     48.230      0.000       1.174       1.274\n",
       "horse_number-6                           1.4018      0.026     54.395      0.000       1.351       1.452\n",
       "horse_number-8                           1.1470      0.025     45.660      0.000       1.098       1.196\n",
       "horse_number-9                           0.9919      0.025     39.892      0.000       0.943       1.041\n",
       "horse_number-10                          0.9243      0.025     37.386      0.000       0.876       0.973\n",
       "horse_number-11                          0.7739      0.025     31.487      0.000       0.726       0.822\n",
       "horse_number-12                          0.7134      0.024     29.136      0.000       0.665       0.761\n",
       "horse_number-13                          0.5883      0.024     24.045      0.000       0.540       0.636\n",
       "horse_number-14                          0.5067      0.024     20.699      0.000       0.459       0.555\n",
       "horse_number-15                          0.3311      0.024     13.697      0.000       0.284       0.379\n",
       "horse_number-16                          0.2038      0.025      8.313      0.000       0.156       0.252\n",
       "horse_number-17                          0.0288      0.030      0.944      0.345      -0.031       0.089\n",
       "time-10-12                               0.3385      0.005     69.792      0.000       0.329       0.348\n",
       "time-15-after                           -0.1584      0.004    -36.383      0.000      -0.167      -0.150\n",
       "place_of_birth_jockey-outside_tokyo     -0.0580      0.007     -8.662      0.000      -0.071      -0.045\n",
       "place_of_birth_trainer-outside_tokyo    -0.0353      0.006     -6.183      0.000      -0.047      -0.024\n",
       "last_run_time                            0.3181      0.001    365.981      0.000       0.316       0.320\n",
       "==============================================================================\n",
       "Omnibus:                   189316.328   Durbin-Watson:                   1.811\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          3523847.206\n",
       "Skew:                           0.709   Prob(JB):                         0.00\n",
       "Kurtosis:                      13.375   Cond. No.                     3.85e+15\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.23e-20. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horse_race_df_grp_by = horse_race_df.set_index(['horse_id', 'run_date'])\n",
    "horse_race_df_grp_by['run_time_diff'] = horse_race_df_grp_by['run_time_1000'].diff()\n",
    "horse_race_df_grp_by = horse_race_df_grp_by[~horse_race_df_grp_by.index.isin(first_occur_df.set_index(['horse_id', \n",
    "                                                                                                       'run_date']).index)]\n",
    "horse_race_df_grp_by.reset_index(inplace=True)\n",
    "new_df_full_diff = feature_engineer(horse_race_df_grp_by)\n",
    "new_df_full_diff['last_run_time'] = new_df_full_diff['run_time_1000'] - new_df_full_diff['run_time_diff']\n",
    "new_df_full_diff.drop('run_time_diff', inplace=True, axis=1)\n",
    "\n",
    "X_full_diff = new_df_full_diff.loc[:, new_df_full_diff.columns != 'run_time_1000']\n",
    "y_full_diff = new_df_full_diff.loc[:, 'run_time_1000']\n",
    "X_full_diff = sm.add_constant(X_full_diff)\n",
    "results_diff = sm.OLS(y_full_diff, X_full_diff).fit()\n",
    "results_diff.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_window = 3\n",
    "try:\n",
    "    df_combined = pd.read_csv('data/horse_race_combined.csv', low_memory=False, index_col=0)\n",
    "    df_combined['run_date'] = df_combined['run_date'].apply(lambda x: pd.Timestamp(x))\n",
    "    df_combined = df_combined.sort_values(['horse_id', 'run_date'])\n",
    "    \n",
    "    df_combined = df_combined.iloc[:5000]\n",
    "    \n",
    "    df_combined.set_index(['horse_id', 'run_date'], inplace=True)\n",
    "    df_combined = df_combined[~df_combined.index.isin(first_occur_df.sort_values(['horse_id', 'run_date']).set_index(['horse_id', 'run_date']).index)]\n",
    "except FileNotFoundError:\n",
    "    df_combined = horse_race_df.set_index(['horse_id', 'run_date'])\n",
    "    df_combined['run_time_diff'] = df_combined['run_time_1000'].diff()\n",
    "    df_combined['last_run_time'] = df_combined['run_time_1000'] - df_combined['run_time_diff']\n",
    "    df_combined['run_time_quo'] = df_combined['run_time_1000'] / df_combined['last_run_time']\n",
    "    df_combined['run_time_mean'] = df_combined['last_run_time']\n",
    "\n",
    "    df_reset = df_combined['run_time_mean'].reset_index()\n",
    "    horse_id_lst = list(df_reset['horse_id'])\n",
    "    run_time_mean_lst = list(df_reset['run_time_mean'])\n",
    "    new_run_time_mean_lst = []\n",
    "    new_run_time_median_lst = []\n",
    "    curr_index = horse_id_lst[0]\n",
    "    curr_count = 0\n",
    "    curr_sum = 0\n",
    "    curr_stored = []\n",
    "    for index, value in zip(horse_id_lst, run_time_mean_lst):\n",
    "        if index != curr_index:\n",
    "            curr_count = 1\n",
    "            curr_sum = value\n",
    "            curr_index = index\n",
    "            curr_stored = [value]\n",
    "        else:\n",
    "            curr_count += 1\n",
    "            curr_sum += value\n",
    "            curr_stored.append(value)\n",
    "        new_run_time_mean_lst.append(curr_sum / (curr_count * 1.0))\n",
    "        new_run_time_median_lst.append(np.median(curr_stored))\n",
    "    df_combined['run_time_mean'] = pd.Series(new_run_time_mean_lst, index=df_combined.index)\n",
    "    df_combined['run_time_median'] = pd.Series(new_run_time_median_lst, index=df_combined.index)\n",
    "\n",
    "    for window in range(2, max_window + 1):\n",
    "        ma = df_combined.groupby(level=0)['run_time_1000'].rolling(window).mean().groupby(level=0).shift(1)\n",
    "        ma = ma.reset_index(level=1)['run_time_1000'].reset_index()\n",
    "        ewma = df_combined.groupby(level=0)['run_time_1000'].apply(lambda series: series.ewm(ignore_na=True, \n",
    "                                                                                             min_periods=window, \n",
    "                                                                                             adjust=True,\n",
    "                                                                                             com=0.030927835051546).mean())\n",
    "        ewma = ewma.groupby(level=0).shift(1)\n",
    "        df_combined['run_time_ma_window_%s' % str(window)] = ma.set_index(['horse_id', 'run_date'])['run_time_1000']\n",
    "        df_combined['run_time_ewma_window_%s' % str(window)] = ewma\n",
    "    df_combined.reset_index().to_csv('data/horse_race_combined.csv', encoding='utf-8')\n",
    "\n",
    "dependent = ['run_time_1000',\n",
    "             'run_time_diff', 'run_time_quo', \n",
    "             'run_time_mean', 'run_time_median'] + \\\n",
    "            ['run_time_ma_window_%s' % str(idx) for idx in range(2, max_window + 1)] + \\\n",
    "            ['run_time_ewma_window_%s' % str(idx) for idx in range(2, max_window + 1)]\n",
    "df_combined_y = df_combined[dependent].copy()\n",
    "df_combined_x = df_combined[list(filter(lambda x: x not in dependent, df_combined.columns))].copy()\n",
    "df_y_original_dict = {}\n",
    "df_y_original_dict['run_time_diff'] = df_combined_x['last_run_time']\n",
    "df_y_original_dict['run_time_quo'] = df_combined_x['last_run_time']\n",
    "for col_name in dependent[3:]:\n",
    "    df_combined_y[col_name + '_diff'] = df_combined_y['run_time_1000'] - df_combined_y[col_name]\n",
    "    df_combined_y[col_name + '_quo'] = df_combined_y['run_time_1000'] / df_combined_y[col_name]\n",
    "    df_y_original_dict[col_name + '_diff'] = df_combined_y[col_name]\n",
    "    df_y_original_dict[col_name + '_quo'] = df_combined_y[col_name]\n",
    "    df_combined_y.drop(col_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>horse_id</th>\n",
       "      <th>1986102130</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1989107128</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1990104469</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1991100019</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1991103654</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1995100680</th>\n",
       "      <th colspan=\"7\" halign=\"left\">1995100686</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_date</th>\n",
       "      <th>2000-01-29 15:40:00</th>\n",
       "      <th>2000-02-05 13:45:00</th>\n",
       "      <th>2000-03-11 15:10:00</th>\n",
       "      <th>2000-04-15 15:30:00</th>\n",
       "      <th>2000-03-18 11:45:00</th>\n",
       "      <th>2000-04-15 15:30:00</th>\n",
       "      <th>2000-02-12 15:45:00</th>\n",
       "      <th>2000-03-19 15:00:00</th>\n",
       "      <th>2000-02-26 11:50:00</th>\n",
       "      <th>2000-03-11 15:10:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2001-02-24 11:50:00</th>\n",
       "      <th>2001-03-10 11:50:00</th>\n",
       "      <th>2001-03-25 11:45:00</th>\n",
       "      <th>2000-07-02 15:45:00</th>\n",
       "      <th>2000-07-30 15:45:00</th>\n",
       "      <th>2000-09-02 15:45:00</th>\n",
       "      <th>2000-09-16 15:45:00</th>\n",
       "      <th>2000-10-21 15:25:00</th>\n",
       "      <th>2000-11-11 15:40:00</th>\n",
       "      <th>2000-11-19 15:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>run_time_diff</th>\n",
       "      <td>1.642857</td>\n",
       "      <td>-1.146258</td>\n",
       "      <td>0.510490</td>\n",
       "      <td>2.697936</td>\n",
       "      <td>1.279356</td>\n",
       "      <td>1.830030</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>-1.160714</td>\n",
       "      <td>-1.567140</td>\n",
       "      <td>0.568839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.642812</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>-2.444444</td>\n",
       "      <td>-0.950000</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>-3.708333</td>\n",
       "      <td>0.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_quo</th>\n",
       "      <td>1.027381</td>\n",
       "      <td>0.983224</td>\n",
       "      <td>1.007599</td>\n",
       "      <td>1.039856</td>\n",
       "      <td>1.019112</td>\n",
       "      <td>1.026826</td>\n",
       "      <td>1.003567</td>\n",
       "      <td>0.980746</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>1.008398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990745</td>\n",
       "      <td>1.017761</td>\n",
       "      <td>0.965098</td>\n",
       "      <td>0.984413</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>1.020446</td>\n",
       "      <td>0.975410</td>\n",
       "      <td>1.056022</td>\n",
       "      <td>0.940981</td>\n",
       "      <td>1.012920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_mean_diff</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.547192</td>\n",
       "      <td>2.208618</td>\n",
       "      <td>4.354399</td>\n",
       "      <td>-0.446069</td>\n",
       "      <td>1.532651</td>\n",
       "      <td>-4.396341</td>\n",
       "      <td>-4.091609</td>\n",
       "      <td>1.722220</td>\n",
       "      <td>1.716986</td>\n",
       "      <td>...</td>\n",
       "      <td>6.201993</td>\n",
       "      <td>6.860398</td>\n",
       "      <td>3.844254</td>\n",
       "      <td>-4.271296</td>\n",
       "      <td>-3.069753</td>\n",
       "      <td>-1.080093</td>\n",
       "      <td>-2.364074</td>\n",
       "      <td>1.363272</td>\n",
       "      <td>-2.539815</td>\n",
       "      <td>-1.458449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_mean_quo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.039409</td>\n",
       "      <td>1.033728</td>\n",
       "      <td>1.065940</td>\n",
       "      <td>0.993504</td>\n",
       "      <td>1.022369</td>\n",
       "      <td>0.932032</td>\n",
       "      <td>0.935276</td>\n",
       "      <td>1.026088</td>\n",
       "      <td>1.025784</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099053</td>\n",
       "      <td>1.108591</td>\n",
       "      <td>1.060304</td>\n",
       "      <td>0.933543</td>\n",
       "      <td>0.951156</td>\n",
       "      <td>0.982602</td>\n",
       "      <td>0.961786</td>\n",
       "      <td>1.022178</td>\n",
       "      <td>0.958813</td>\n",
       "      <td>0.976226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_median_diff</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.547192</td>\n",
       "      <td>0.510490</td>\n",
       "      <td>2.953181</td>\n",
       "      <td>-0.446069</td>\n",
       "      <td>1.830030</td>\n",
       "      <td>-4.396341</td>\n",
       "      <td>-1.160714</td>\n",
       "      <td>1.722220</td>\n",
       "      <td>0.568839</td>\n",
       "      <td>...</td>\n",
       "      <td>7.464815</td>\n",
       "      <td>8.537037</td>\n",
       "      <td>5.842593</td>\n",
       "      <td>-4.271296</td>\n",
       "      <td>-1.172222</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>-1.450000</td>\n",
       "      <td>2.358333</td>\n",
       "      <td>-1.825000</td>\n",
       "      <td>-0.586111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_median_quo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.039409</td>\n",
       "      <td>1.007599</td>\n",
       "      <td>1.043792</td>\n",
       "      <td>0.993504</td>\n",
       "      <td>1.026826</td>\n",
       "      <td>0.932032</td>\n",
       "      <td>0.980746</td>\n",
       "      <td>1.026088</td>\n",
       "      <td>1.008398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.121676</td>\n",
       "      <td>1.138814</td>\n",
       "      <td>1.094617</td>\n",
       "      <td>0.933543</td>\n",
       "      <td>0.980767</td>\n",
       "      <td>1.008681</td>\n",
       "      <td>0.976210</td>\n",
       "      <td>1.038997</td>\n",
       "      <td>0.970057</td>\n",
       "      <td>0.990308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ma_window_2_diff</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.062639</td>\n",
       "      <td>2.953181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.469709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.053571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.214731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.850707</td>\n",
       "      <td>0.900816</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.697222</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>-2.041667</td>\n",
       "      <td>-1.090278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ma_window_2_quo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>1.043792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.036545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027637</td>\n",
       "      <td>1.013030</td>\n",
       "      <td>0.973593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.988471</td>\n",
       "      <td>1.018553</td>\n",
       "      <td>0.985281</td>\n",
       "      <td>1.042877</td>\n",
       "      <td>0.966621</td>\n",
       "      <td>0.982120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ma_window_3_diff</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.656177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.905410</td>\n",
       "      <td>2.456027</td>\n",
       "      <td>-1.843900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>-0.759259</td>\n",
       "      <td>2.740741</td>\n",
       "      <td>-1.986111</td>\n",
       "      <td>-0.597222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ma_window_3_quo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.039215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.060167</td>\n",
       "      <td>1.036342</td>\n",
       "      <td>0.973445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.012573</td>\n",
       "      <td>0.987400</td>\n",
       "      <td>1.045609</td>\n",
       "      <td>0.967500</td>\n",
       "      <td>0.990126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ewma_window_2_diff</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477103</td>\n",
       "      <td>2.712237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.867293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.154473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.489916</td>\n",
       "      <td>1.207525</td>\n",
       "      <td>-2.408219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.249892</td>\n",
       "      <td>1.214732</td>\n",
       "      <td>-1.463559</td>\n",
       "      <td>3.289427</td>\n",
       "      <td>-3.609651</td>\n",
       "      <td>0.655599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ewma_window_2_quo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.007098</td>\n",
       "      <td>1.040076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.027387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.007718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992931</td>\n",
       "      <td>1.017544</td>\n",
       "      <td>0.965597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995837</td>\n",
       "      <td>1.020318</td>\n",
       "      <td>0.975993</td>\n",
       "      <td>1.055244</td>\n",
       "      <td>0.942462</td>\n",
       "      <td>1.011068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ewma_window_3_diff</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.712237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.489916</td>\n",
       "      <td>1.207525</td>\n",
       "      <td>-2.408219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.214732</td>\n",
       "      <td>-1.463559</td>\n",
       "      <td>3.289427</td>\n",
       "      <td>-3.609651</td>\n",
       "      <td>0.655599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_time_ewma_window_3_quo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.040076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992931</td>\n",
       "      <td>1.017544</td>\n",
       "      <td>0.965597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.020318</td>\n",
       "      <td>0.975993</td>\n",
       "      <td>1.055244</td>\n",
       "      <td>0.942462</td>\n",
       "      <td>1.011068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 4340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "horse_id                             1986102130          1989107128  \\\n",
       "run_date                    2000-01-29 15:40:00 2000-02-05 13:45:00   \n",
       "run_time_diff                          1.642857           -1.146258   \n",
       "run_time_quo                           1.027381            0.983224   \n",
       "run_time_mean_diff                          NaN            2.547192   \n",
       "run_time_mean_quo                           NaN            1.039409   \n",
       "run_time_median_diff                        NaN            2.547192   \n",
       "run_time_median_quo                         NaN            1.039409   \n",
       "run_time_ma_window_2_diff                   NaN                 NaN   \n",
       "run_time_ma_window_2_quo                    NaN                 NaN   \n",
       "run_time_ma_window_3_diff                   NaN                 NaN   \n",
       "run_time_ma_window_3_quo                    NaN                 NaN   \n",
       "run_time_ewma_window_2_diff                 NaN                 NaN   \n",
       "run_time_ewma_window_2_quo                  NaN                 NaN   \n",
       "run_time_ewma_window_3_diff                 NaN                 NaN   \n",
       "run_time_ewma_window_3_quo                  NaN                 NaN   \n",
       "\n",
       "horse_id                                                             \\\n",
       "run_date                    2000-03-11 15:10:00 2000-04-15 15:30:00   \n",
       "run_time_diff                          0.510490            2.697936   \n",
       "run_time_quo                           1.007599            1.039856   \n",
       "run_time_mean_diff                     2.208618            4.354399   \n",
       "run_time_mean_quo                      1.033728            1.065940   \n",
       "run_time_median_diff                   0.510490            2.953181   \n",
       "run_time_median_quo                    1.007599            1.043792   \n",
       "run_time_ma_window_2_diff             -0.062639            2.953181   \n",
       "run_time_ma_window_2_quo               0.999076            1.043792   \n",
       "run_time_ma_window_3_diff                   NaN            2.656177   \n",
       "run_time_ma_window_3_quo                    NaN            1.039215   \n",
       "run_time_ewma_window_2_diff            0.477103            2.712237   \n",
       "run_time_ewma_window_2_quo             1.007098            1.040076   \n",
       "run_time_ewma_window_3_diff                 NaN            2.712237   \n",
       "run_time_ewma_window_3_quo                  NaN            1.040076   \n",
       "\n",
       "horse_id                             1990104469                      \\\n",
       "run_date                    2000-03-18 11:45:00 2000-04-15 15:30:00   \n",
       "run_time_diff                          1.279356            1.830030   \n",
       "run_time_quo                           1.019112            1.026826   \n",
       "run_time_mean_diff                    -0.446069            1.532651   \n",
       "run_time_mean_quo                      0.993504            1.022369   \n",
       "run_time_median_diff                  -0.446069            1.830030   \n",
       "run_time_median_quo                    0.993504            1.026826   \n",
       "run_time_ma_window_2_diff                   NaN            2.469709   \n",
       "run_time_ma_window_2_quo                    NaN            1.036545   \n",
       "run_time_ma_window_3_diff                   NaN                 NaN   \n",
       "run_time_ma_window_3_quo                    NaN                 NaN   \n",
       "run_time_ewma_window_2_diff                 NaN            1.867293   \n",
       "run_time_ewma_window_2_quo                  NaN            1.027387   \n",
       "run_time_ewma_window_3_diff                 NaN                 NaN   \n",
       "run_time_ewma_window_3_quo                  NaN                 NaN   \n",
       "\n",
       "horse_id                             1991100019                      \\\n",
       "run_date                    2000-02-12 15:45:00 2000-03-19 15:00:00   \n",
       "run_time_diff                          0.214286           -1.160714   \n",
       "run_time_quo                           1.003567            0.980746   \n",
       "run_time_mean_diff                    -4.396341           -4.091609   \n",
       "run_time_mean_quo                      0.932032            0.935276   \n",
       "run_time_median_diff                  -4.396341           -1.160714   \n",
       "run_time_median_quo                    0.932032            0.980746   \n",
       "run_time_ma_window_2_diff                   NaN           -1.053571   \n",
       "run_time_ma_window_2_quo                    NaN            0.982493   \n",
       "run_time_ma_window_3_diff                   NaN                 NaN   \n",
       "run_time_ma_window_3_quo                    NaN                 NaN   \n",
       "run_time_ewma_window_2_diff                 NaN           -1.154473   \n",
       "run_time_ewma_window_2_quo                  NaN            0.980848   \n",
       "run_time_ewma_window_3_diff                 NaN                 NaN   \n",
       "run_time_ewma_window_3_quo                  NaN                 NaN   \n",
       "\n",
       "horse_id                             1991103654                      \\\n",
       "run_date                    2000-02-26 11:50:00 2000-03-11 15:10:00   \n",
       "run_time_diff                         -1.567140            0.568839   \n",
       "run_time_quo                           0.977388            1.008398   \n",
       "run_time_mean_diff                     1.722220            1.716986   \n",
       "run_time_mean_quo                      1.026088            1.025784   \n",
       "run_time_median_diff                   1.722220            0.568839   \n",
       "run_time_median_quo                    1.026088            1.008398   \n",
       "run_time_ma_window_2_diff                   NaN           -0.214731   \n",
       "run_time_ma_window_2_quo                    NaN            0.996866   \n",
       "run_time_ma_window_3_diff                   NaN                 NaN   \n",
       "run_time_ma_window_3_quo                    NaN                 NaN   \n",
       "run_time_ewma_window_2_diff                 NaN            0.523194   \n",
       "run_time_ewma_window_2_quo                  NaN            1.007718   \n",
       "run_time_ewma_window_3_diff                 NaN                 NaN   \n",
       "run_time_ewma_window_3_quo                  NaN                 NaN   \n",
       "\n",
       "horse_id                            ...                  1995100680  \\\n",
       "run_date                            ...         2001-02-24 11:50:00   \n",
       "run_time_diff                       ...                   -0.642812   \n",
       "run_time_quo                        ...                    0.990745   \n",
       "run_time_mean_diff                  ...                    6.201993   \n",
       "run_time_mean_quo                   ...                    1.099053   \n",
       "run_time_median_diff                ...                    7.464815   \n",
       "run_time_median_quo                 ...                    1.121676   \n",
       "run_time_ma_window_2_diff           ...                    1.850707   \n",
       "run_time_ma_window_2_quo            ...                    1.027637   \n",
       "run_time_ma_window_3_diff           ...                    3.905410   \n",
       "run_time_ma_window_3_quo            ...                    1.060167   \n",
       "run_time_ewma_window_2_diff         ...                   -0.489916   \n",
       "run_time_ewma_window_2_quo          ...                    0.992931   \n",
       "run_time_ewma_window_3_diff         ...                   -0.489916   \n",
       "run_time_ewma_window_3_quo          ...                    0.992931   \n",
       "\n",
       "horse_id                                                             \\\n",
       "run_date                    2001-03-10 11:50:00 2001-03-25 11:45:00   \n",
       "run_time_diff                          1.222222           -2.444444   \n",
       "run_time_quo                           1.017761            0.965098   \n",
       "run_time_mean_diff                     6.860398            3.844254   \n",
       "run_time_mean_quo                      1.108591            1.060304   \n",
       "run_time_median_diff                   8.537037            5.842593   \n",
       "run_time_median_quo                    1.138814            1.094617   \n",
       "run_time_ma_window_2_diff              0.900816           -1.833333   \n",
       "run_time_ma_window_2_quo               1.013030            0.973593   \n",
       "run_time_ma_window_3_diff              2.456027           -1.843900   \n",
       "run_time_ma_window_3_quo               1.036342            0.973445   \n",
       "run_time_ewma_window_2_diff            1.207525           -2.408219   \n",
       "run_time_ewma_window_2_quo             1.017544            0.965597   \n",
       "run_time_ewma_window_3_diff            1.207525           -2.408219   \n",
       "run_time_ewma_window_3_quo             1.017544            0.965597   \n",
       "\n",
       "horse_id                             1995100686                      \\\n",
       "run_date                    2000-07-02 15:45:00 2000-07-30 15:45:00   \n",
       "run_time_diff                         -0.950000           -0.222222   \n",
       "run_time_quo                           0.984413            0.996296   \n",
       "run_time_mean_diff                    -4.271296           -3.069753   \n",
       "run_time_mean_quo                      0.933543            0.951156   \n",
       "run_time_median_diff                  -4.271296           -1.172222   \n",
       "run_time_median_quo                    0.933543            0.980767   \n",
       "run_time_ma_window_2_diff                   NaN           -0.697222   \n",
       "run_time_ma_window_2_quo                    NaN            0.988471   \n",
       "run_time_ma_window_3_diff                   NaN                 NaN   \n",
       "run_time_ma_window_3_quo                    NaN                 NaN   \n",
       "run_time_ewma_window_2_diff                 NaN           -0.249892   \n",
       "run_time_ewma_window_2_quo                  NaN            0.995837   \n",
       "run_time_ewma_window_3_diff                 NaN                 NaN   \n",
       "run_time_ewma_window_3_quo                  NaN                 NaN   \n",
       "\n",
       "horse_id                                                             \\\n",
       "run_date                    2000-09-02 15:45:00 2000-09-16 15:45:00   \n",
       "run_time_diff                          1.222222           -1.500000   \n",
       "run_time_quo                           1.020446            0.975410   \n",
       "run_time_mean_diff                    -1.080093           -2.364074   \n",
       "run_time_mean_quo                      0.982602            0.961786   \n",
       "run_time_median_diff                   0.525000           -1.450000   \n",
       "run_time_median_quo                    1.008681            0.976210   \n",
       "run_time_ma_window_2_diff              1.111111           -0.888889   \n",
       "run_time_ma_window_2_quo               1.018553            0.985281   \n",
       "run_time_ma_window_3_diff              0.757407           -0.759259   \n",
       "run_time_ma_window_3_quo               1.012573            0.987400   \n",
       "run_time_ewma_window_2_diff            1.214732           -1.463559   \n",
       "run_time_ewma_window_2_quo             1.020318            0.975993   \n",
       "run_time_ewma_window_3_diff            1.214732           -1.463559   \n",
       "run_time_ewma_window_3_quo             1.020318            0.975993   \n",
       "\n",
       "horse_id                                                             \\\n",
       "run_date                    2000-10-21 15:25:00 2000-11-11 15:40:00   \n",
       "run_time_diff                          3.333333           -3.708333   \n",
       "run_time_quo                           1.056022            0.940981   \n",
       "run_time_mean_diff                     1.363272           -2.539815   \n",
       "run_time_mean_quo                      1.022178            0.958813   \n",
       "run_time_median_diff                   2.358333           -1.825000   \n",
       "run_time_median_quo                    1.038997            0.970057   \n",
       "run_time_ma_window_2_diff              2.583333           -2.041667   \n",
       "run_time_ma_window_2_quo               1.042877            0.966621   \n",
       "run_time_ma_window_3_diff              2.740741           -1.986111   \n",
       "run_time_ma_window_3_quo               1.045609            0.967500   \n",
       "run_time_ewma_window_2_diff            3.289427           -3.609651   \n",
       "run_time_ewma_window_2_quo             1.055244            0.942462   \n",
       "run_time_ewma_window_3_diff            3.289427           -3.609651   \n",
       "run_time_ewma_window_3_quo             1.055244            0.942462   \n",
       "\n",
       "horse_id                                         \n",
       "run_date                    2000-11-19 15:00:00  \n",
       "run_time_diff                          0.763889  \n",
       "run_time_quo                           1.012920  \n",
       "run_time_mean_diff                    -1.458449  \n",
       "run_time_mean_quo                      0.976226  \n",
       "run_time_median_diff                  -0.586111  \n",
       "run_time_median_quo                    0.990308  \n",
       "run_time_ma_window_2_diff             -1.090278  \n",
       "run_time_ma_window_2_quo               0.982120  \n",
       "run_time_ma_window_3_diff             -0.597222  \n",
       "run_time_ma_window_3_quo               0.990126  \n",
       "run_time_ewma_window_2_diff            0.655599  \n",
       "run_time_ewma_window_2_quo             1.011068  \n",
       "run_time_ewma_window_3_diff            0.655599  \n",
       "run_time_ewma_window_3_quo             1.011068  \n",
       "\n",
       "[14 rows x 4340 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_y[list(filter(lambda x: 'diff' in x or 'quo' in x, df_combined_y.columns))].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for OLS: 2.01725\n"
     ]
    }
   ],
   "source": [
    "# Model testing for run time residual\n",
    "x = feature_engineer(df_combined_x.reset_index())\n",
    "x = x.drop('last_run_time', axis=1)\n",
    "y = df_combined_y.loc[df_combined_y.index.isin(x.index), 'run_time_diff']\n",
    "\n",
    "# OLS\n",
    "reg = linear_model.LinearRegression(fit_intercept=False)\n",
    "scores_reg = cross_val_score(reg, x, y, scoring='neg_mean_squared_error')\n",
    "print(\"RMSE for OLS: %0.5f\" % np.sqrt(-scores_reg.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparer(object): \n",
    "    \n",
    "    no_drop_col = ['run_time_1000']\n",
    "    \n",
    "    def __init__(self, X_df, y_df, original_y_df_dict):\n",
    "        self.X = feature_engineer(X_df.reset_index())\n",
    "        self.y = y_df[y_df.index.isin(self.X.index)]\n",
    "        self.y_original = original_y_df_dict\n",
    "        self.run_time_serie = self.y['run_time_1000']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.3)\n",
    "        self.model_dict = {}\n",
    "        \n",
    "    def add_model(self, model_method, model_name):\n",
    "        \n",
    "        print('Adding model named %s ' % model_name)\n",
    "        self.model_dict[model_name] = {}\n",
    "        \n",
    "        for y_col_name in self.y.columns:\n",
    "            X_train = self.X_train.copy()\n",
    "            X_test = self.X_test.copy()                \n",
    "            model = model_method\n",
    "            self.model_dict[model_name]['Model Spec'] = repr(model)\n",
    "            \n",
    "            if y_col_name not in self.no_drop_col:\n",
    "                X_train.drop('last_run_time', axis=1, inplace=True)\n",
    "                X_test.drop('last_run_time', axis=1, inplace=True)\n",
    "                \n",
    "            y_train = self.y_train[y_col_name].dropna()\n",
    "            y_test = self.y_test[y_col_name].dropna()\n",
    "            \n",
    "            X_train = X_train[X_train.index.isin(y_train.index)]\n",
    "            X_test = X_test[X_test.index.isin(y_test.index)]\n",
    "                     \n",
    "            if 'normalized' in model_name.lower():\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X_train)\n",
    "                X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index)\n",
    "                X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index)\n",
    "            \n",
    "            print('Performing analysis on column %s for model %s (Size: %s)' % (y_col_name, model_name, str(X_train.shape)))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            if y_col_name not in self.no_drop_col:\n",
    "                y_pred = pd.Series(y_pred, index=X_test.index)\n",
    "                original_serie = df_y_original_dict[y_col_name]\n",
    "                original_serie = original_serie[original_serie.index.isin(y_pred.index)]\n",
    "                run_time_serie = self.run_time_serie[self.run_time_serie.index.isin(y_pred.index)]\n",
    "                if 'quo' in y_col_name:\n",
    "                    y_pred = y_pred * original_serie\n",
    "                elif 'diff' in y_col_name:\n",
    "                    y_pred = y_pred + original_serie\n",
    "                self.model_dict[model_name]['Transformed RMSE: %s (%s)' % (y_col_name, \n",
    "                                                                           X_train.shape[0])] = '%.6f' % self.get_rmse(y_pred, \n",
    "                                                                                                                       run_time_serie)  \n",
    "            else:\n",
    "                self.model_dict[model_name]['Original RMSE: %s (%s)' % (y_col_name, \n",
    "                                                                        X_train.shape[0])] = '%.6f' % self.get_rmse(y_pred, \n",
    "                                                                                                                    y_test)\n",
    "        \n",
    "    def get_report(self):\n",
    "        try:\n",
    "            df = pd.DataFrame.from_dict(self.model_dict)\n",
    "            return df.sort_values(df.columns[0])\n",
    "        except IndexError:\n",
    "            return\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_rmse(y_true, y_pred):\n",
    "        diff = np.sum((y_true - y_pred) ** 2)\n",
    "        return (diff / y_true.shape[0]) ** 1/2\n",
    "    \n",
    "mc = ModelComparer(df_combined_x, df_combined_y, df_y_original_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding model named OLS - Base Model \n",
      "Performing analysis on column run_time_1000 for model OLS - Base Model (Size: (3038, 67))\n",
      "Performing analysis on column run_time_diff for model OLS - Base Model (Size: (3038, 66))\n",
      "Performing analysis on column run_time_quo for model OLS - Base Model (Size: (3038, 66))\n",
      "Performing analysis on column run_time_mean_diff for model OLS - Base Model (Size: (3037, 66))\n",
      "Performing analysis on column run_time_mean_quo for model OLS - Base Model (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_diff for model OLS - Base Model (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_quo for model OLS - Base Model (Size: (3037, 66))\n",
      "Performing analysis on column run_time_ma_window_2_diff for model OLS - Base Model (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_2_quo for model OLS - Base Model (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_3_diff for model OLS - Base Model (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ma_window_3_quo for model OLS - Base Model (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_diff for model OLS - Base Model (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_quo for model OLS - Base Model (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_diff for model OLS - Base Model (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_quo for model OLS - Base Model (Size: (2325, 66))\n",
      "Adding model named XGB - Base Model (0.1LR) \n",
      "Performing analysis on column run_time_1000 for model XGB - Base Model (0.1LR) (Size: (3038, 67))\n",
      "Performing analysis on column run_time_diff for model XGB - Base Model (0.1LR) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_quo for model XGB - Base Model (0.1LR) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_mean_diff for model XGB - Base Model (0.1LR) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_mean_quo for model XGB - Base Model (0.1LR) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_diff for model XGB - Base Model (0.1LR) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_quo for model XGB - Base Model (0.1LR) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_ma_window_2_diff for model XGB - Base Model (0.1LR) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_2_quo for model XGB - Base Model (0.1LR) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_3_diff for model XGB - Base Model (0.1LR) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ma_window_3_quo for model XGB - Base Model (0.1LR) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_diff for model XGB - Base Model (0.1LR) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_quo for model XGB - Base Model (0.1LR) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_diff for model XGB - Base Model (0.1LR) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_quo for model XGB - Base Model (0.1LR) (Size: (2325, 66))\n",
      "Adding model named DT - Base Model (6MD) \n",
      "Performing analysis on column run_time_1000 for model DT - Base Model (6MD) (Size: (3038, 67))\n",
      "Performing analysis on column run_time_diff for model DT - Base Model (6MD) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_quo for model DT - Base Model (6MD) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_mean_diff for model DT - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_mean_quo for model DT - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_diff for model DT - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_quo for model DT - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_ma_window_2_diff for model DT - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_2_quo for model DT - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_3_diff for model DT - Base Model (6MD) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ma_window_3_quo for model DT - Base Model (6MD) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_diff for model DT - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_quo for model DT - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_diff for model DT - Base Model (6MD) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_quo for model DT - Base Model (6MD) (Size: (2325, 66))\n",
      "Adding model named RF - Base Model (6MD) \n",
      "Performing analysis on column run_time_1000 for model RF - Base Model (6MD) (Size: (3038, 67))\n",
      "Performing analysis on column run_time_diff for model RF - Base Model (6MD) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_quo for model RF - Base Model (6MD) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_mean_diff for model RF - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_mean_quo for model RF - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_diff for model RF - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_quo for model RF - Base Model (6MD) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_ma_window_2_diff for model RF - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_2_quo for model RF - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ma_window_3_diff for model RF - Base Model (6MD) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ma_window_3_quo for model RF - Base Model (6MD) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_diff for model RF - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_2_quo for model RF - Base Model (6MD) (Size: (2652, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_diff for model RF - Base Model (6MD) (Size: (2325, 66))\n",
      "Performing analysis on column run_time_ewma_window_3_quo for model RF - Base Model (6MD) (Size: (2325, 66))\n",
      "Adding model named GBM - Base Model (6MD, 0.1LR) \n",
      "Performing analysis on column run_time_1000 for model GBM - Base Model (6MD, 0.1LR) (Size: (3038, 67))\n",
      "Performing analysis on column run_time_diff for model GBM - Base Model (6MD, 0.1LR) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_quo for model GBM - Base Model (6MD, 0.1LR) (Size: (3038, 66))\n",
      "Performing analysis on column run_time_mean_diff for model GBM - Base Model (6MD, 0.1LR) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_mean_quo for model GBM - Base Model (6MD, 0.1LR) (Size: (3037, 66))\n",
      "Performing analysis on column run_time_median_diff for model GBM - Base Model (6MD, 0.1LR) (Size: (3037, 66))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bbbed0f4aea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_decision_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DT - Base Model (6MD)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_random_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RF - Base Model (6MD)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_gbm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ls'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GBM - Base Model (6MD, 0.1LR)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_ann\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ANN - Base Model (normalized, 100000MI)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-4e3c9e7d03a6>\u001b[0m in \u001b[0;36madd_model\u001b[1;34m(self, model_method, model_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Performing analysis on column %s for model %s (Size: %s)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_col_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 788\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_ols(**kwargs):\n",
    "    reg = linear_model.LinearRegression(**kwargs)\n",
    "    return reg\n",
    "\n",
    "def get_xgboost(**kwargs):\n",
    "    xgb_model = xgb.XGBRegressor(**kwargs)\n",
    "    return xgb_model\n",
    "\n",
    "def get_decision_tree(**kwargs):\n",
    "    dt = DecisionTreeRegressor(**kwargs)\n",
    "    return dt\n",
    "\n",
    "def get_random_forest(**kwargs):\n",
    "    regr = RandomForestRegressor(**kwargs)\n",
    "    return regr\n",
    "\n",
    "def get_gbm(**kwargs):\n",
    "    clf = GradientBoostingRegressor(**kwargs)\n",
    "    return clf\n",
    "\n",
    "def get_ann(**kwargs):\n",
    "    mlp = MLPRegressor(**kwargs)\n",
    "    return mlp\n",
    "\n",
    "# Add base model\n",
    "mc.add_model(get_ols(fit_intercept=False), 'OLS - Base Model')\n",
    "mc.add_model(get_xgboost(learning_rate=0.1), 'XGB - Base Model (0.1LR)')\n",
    "mc.add_model(get_decision_tree(max_depth=6), 'DT - Base Model (6MD)')\n",
    "mc.add_model(get_random_forest(max_depth=6), 'RF - Base Model (6MD)')\n",
    "mc.add_model(get_gbm(n_estimators=500, max_depth=6, min_samples_split=2, learning_rate=0.1, loss='ls'), 'GBM - Base Model (6MD, 0.1LR)')\n",
    "mc.add_model(get_ann(max_iter=100000), 'ANN - Base Model (normalized, 100000MI)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fd30f4c470>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd30f565c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set some specs for plotting\n",
    "mpl.rcParams['figure.figsize'] = (16.0, 8.0)\n",
    "mpl.style.use('ggplot')\n",
    "\n",
    "value_df = mc.get_report().dropna().iloc[:-1, :]\n",
    "value_df = value_df.applymap(lambda x: float(x))\n",
    "sns.heatmap(value_df, annot=True, fmt='g', cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
